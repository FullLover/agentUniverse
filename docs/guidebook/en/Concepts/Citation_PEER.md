### Citation
The agentUniverse project is supported by the following research achievements.

BibTeX formatted
```text
@misc{wang2024peerexpertizingdomainspecifictasks,
      title={PEER: Expertizing Domain-Specific Tasks with a Multi-Agent Framework and Tuning Methods}, 
      author={Yiying Wang and Xiaojing Li and Binzhu Wang and Yueyang Zhou and Han Ji and Hong Chen and Jinshi Zhang and Fei Yu and Zewei Zhao and Song Jin and Renji Gong and Wanqing Xu},
      year={2024},
      eprint={2407.06985},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.06985}, 
}
```
Overview: In the experimental section of this study, scores were assigned across seven dimensions: completeness, relevance, conciseness, factual accuracy, logical coherence, structure, and comprehensiveness (with each dimension having a maximum score of 5 points). On average, the PEER model scored higher in each evaluation dimension compared to BabyAGI, demonstrating significant advantages particularly in the dimensions of completeness, relevance, logical coherence, structure, and comprehensiveness. Furthermore, the PEER model outperformed BabyAGI by an 83% superiority rate when using the GPT-3.5 turbo (16k) model, and by an 81% superiority rate when using the GPT-4.0 model. For further details, please refer to the relevant literature.
https://arxiv.org/pdf/2407.06985